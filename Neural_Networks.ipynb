{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "czCd2JW8U7o7",
        "4zmncgl0VESD",
        "BotNBV_TVjSK",
        "HNdvKjU7KL-v",
        "JxR7s6PeKSvy",
        "2T4v93r4LGMG",
        "vNJgU7lcKco_",
        "3_CTvJcVWNO-",
        "ILJymRTnYXKc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# B-cos Networks: Alignment is All We Need for Interpretability"
      ],
      "metadata": {
        "id": "aAf1mf4gKKup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In this notebook, we present a new direction for increasing the interpretability of Deep Neural Networks (DNNs) by proposing to replace the linear transforms in DNNs by the **B-cos transform**.\n",
        "\n",
        "The B-cos transform is designed to be compatible with existing architectures and\n",
        "we show that it can easily be integrated into common models such as *VGGs*, *ResNets*, *InceptionNets*, and *DenseNets*, whilst maintaining similar performance.\n",
        "\n",
        "The resulting explanations are of high visual quality and perform well under quantitative metrics for interpretability.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "A__EM-rZLrjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "czCd2JW8U7o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.checkpoint as cp\n",
        "import torchvision\n",
        "import warnings\n",
        "\n",
        "from collections import namedtuple, OrderedDict\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
        "from typing import Any, Callable, Dict, List, Optional, Type, Tuple, Union, cast\n",
        "from torch import Tensor\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision import utils\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "HC0N6AeQ3X2N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "4zmncgl0VESD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Datasets**\n",
        "\n",
        "We evaluate the accuracy of several B-cos networks on the *CIFAR-10* dataset.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HLFBwfcIldQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class loadData:\n",
        "    def __init__(self, args):\n",
        "\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.dataPath = args.cifar10Path\n",
        "        self.create_paths(self.dataPath)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_paths(path):\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "    def getDataLoader(self):\n",
        "\n",
        "        cifar10_transforms = transforms.Compose([\n",
        "                                                  transforms.RandomCrop(32, padding=4),\n",
        "                                                  transforms.RandomHorizontalFlip(),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                                 ])\n",
        "\n",
        "        train_loader = DataLoader(torchvision.datasets.CIFAR10(self.dataPath, train=True, download=True, transform=cifar10_transforms), batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        val_loader = DataLoader(torchvision.datasets.CIFAR10(self.dataPath, train=False, download=True, transform=cifar10_transforms), batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        return train_loader, val_loader"
      ],
      "metadata": {
        "id": "34yL-7RwBbUZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "BotNBV_TVjSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**The B-cos transform**\n",
        "\n",
        "Typically, the individual neurons in a DNN compute the dot product between their weights **w** and an input **x**:\n",
        "\n",
        "     f(x; w) = wᵀ x = ||w|| ||x|| c(x, w) with c(x, w) = cos(∠(x, w)).\n",
        "\n",
        "Here, `∠(x, w)` returns the angle between the vectors **x** and **w**.\n",
        "\n",
        "In this work, we seek to improve the interpretability of DNNs by promoting weight-input alignment during optimisation.\n",
        "\n",
        "To achieve this, we propose the ***B-cos transform***:\n",
        "\n",
        "     B-cos(x; w) = ||ŵ|| ||x|| |c(x, ŵ)|ᴮ × sgn (c(x, ŵ)).`\n",
        "\n",
        "Here, *B* is a hyperparameter, the hat-operator scales **ŵ** to unit norm, and `sgn` denotes the *sign* function.\n",
        "\n",
        "Note that this only introduces minor changes with respect to the first equation; e.g., for *B* = 1, the B-cos transform is equivalent to a linear transform with **ŵ**.\n",
        "\n",
        "These changes maintain an important property of the linear transform: similar to sequences of linear transforms, sequences of B-cos transforms can still be faithfully summarised by a single linear transform.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ivvyRwppUfr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormedConv2d(nn.Conv2d):\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight_shape = self.weight.shape\n",
        "\n",
        "        w_hat = self.weight.view(weight_shape[0], -1)\n",
        "        w_hat = w_hat/(w_hat.norm(p=2, dim=1, keepdim=True))\n",
        "        w_hat = w_hat.view(weight_shape)\n",
        "\n",
        "        return F.conv2d(x, w_hat, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "class BcosConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, max_out=2, b=2, scale=None, scale_fact=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.NormedConv2d = NormedConv2d(in_channels, out_channels * max_out, kernel_size, stride, padding, dilation=1, groups=1, bias=False)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.max_out = max_out\n",
        "        self.b = b\n",
        "\n",
        "        if scale is None:\n",
        "            kernel_size_scale = kernel_size if not isinstance(kernel_size, tuple) else np.sqrt(np.prod(kernel_size))\n",
        "            self.scale = (kernel_size_scale * np.sqrt(in_channels)) / scale_fact\n",
        "        else:\n",
        "            self.scale = scale\n",
        "\n",
        "        self.detach = False\n",
        "\n",
        "        self.kernel_size_power = kernel_size**2 if not isinstance(kernel_size, tuple) else np.prod(kernel_size)\n",
        "\n",
        "    def explanation_mode(self, detach=True):\n",
        "        self.detach = detach\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out_normed_conv2d = self.NormedConv2d(x)\n",
        "        batch_size, _, h, w = out_normed_conv2d.shape\n",
        "\n",
        "        # MaxOut computation.\n",
        "        if self.max_out > 1:\n",
        "            out_normed_conv2d = out_normed_conv2d.view(batch_size, -1, self.max_out, h, w)\n",
        "            out_normed_conv2d = out_normed_conv2d.max(dim=2, keepdim=False)[0]\n",
        "\n",
        "        # If B=1, no further calculation necessary.\n",
        "        if self.b == 1:\n",
        "            return out_normed_conv2d / self.scale\n",
        "\n",
        "        # Calculating the norm of input patches.\n",
        "        norm = (F.avg_pool2d((x**2).sum(1, keepdim=True), self.kernel_size, padding=self.padding, stride=self.stride) * self.kernel_size_power + 1e-6).sqrt_()\n",
        "\n",
        "        # Get absolute value of cos.\n",
        "        abs_cos = (out_normed_conv2d/norm).abs() + 1e-6\n",
        "\n",
        "        # In order to compute the explanations.\n",
        "        if self.detach:\n",
        "            abs_cos = abs_cos.detach()\n",
        "\n",
        "        # Additional factor of cos^(b-1).\n",
        "        out_normed_conv2d = out_normed_conv2d * abs_cos.pow(self.b-1)\n",
        "\n",
        "        return out_normed_conv2d / self.scale"
      ],
      "metadata": {
        "id": "X0lmkNE4MGjY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**B-cos networks**\n",
        "\n",
        "The B-cos transform is designed as a *drop-in* replacement of the linear transform, i.e., it can be used in exactly the same way.\n",
        "\n",
        "For example, a conventional fully connected multi-layer neural network f(**x**; θ) of L layers, is represented by:\n",
        "\n",
        "      `f(x; θ) = lL ◦ lL−1 ◦ ... ◦ l2 ◦ l1(x),`\n",
        "\n",
        "with lⱼ denoting layer j with parameters **w**ᵏⱼ for neuron k in layer j, and θ the collection of all model parameters.\n",
        "\n",
        "In such a model, each layer lⱼ typically computes:\n",
        "\n",
        "      `lⱼ(aⱼ; Wⱼ) = φ(Wⱼ aⱼ),`\n",
        "\n",
        "with aⱼ the input to layer j, φ a non-linear activation function (e.g., ReLU), and the row k of Wⱼ given by the weight vector **w**ᵏⱼ of the k-th neuron in that layer.\n",
        "\n",
        "A corresponding **B-cos network** f with layers lⱼ can be formulated in exactly the same way, with the only difference being that every dot product (here between rows of Wⱼ and the input aⱼ) is replaced by the B-cos transform.\n",
        "\n",
        "In matrix form, this equates to:\n",
        "\n",
        "       lⱼ(aⱼ; Wⱼ) = |c(aⱼ; Ŵⱼ)|^(B-1) × (Ŵⱼ aⱼ),`\n",
        "\n",
        "Here, the power, absolute value, and `×` operators are applied element-wise, `c(aⱼ; Ŵⱼ)` computes the cosine similarity between input aⱼ and the rows of Ŵⱼ, and the hat operator scales the rows of Ŵⱼ to unit norm.\n",
        "\n",
        "Finally, note that for *B* > 1 the layer transform lⱼ is non-linear.\n",
        "As a result, a non-linearity φ is not required for a B-cos network to model non-linear relationships.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**MaxOut to increase modelling capacity**\n",
        "\n",
        "As discussed, a deep B-cos network with *B* > 1 does not require a non-linearity between subsequent layers to model non-linear relationships.\n",
        "This, of course, does not mean that it could not benefit from it.\n",
        "\n",
        "In this work, we specifically explore the option of combining the B-cos\n",
        "transform with the MaxOut operation.\n",
        "In particular, we model every neuron in a B-cos network by 2 B-cos transforms of which the maximal activation is forwarded:\n",
        "\n",
        "       MaxOut(x) = max{B-cos(x; wᵢ)} with i∈{1,2},\n",
        "\n",
        "We noticed that networks with the MaxOut operation were much easier to optimise with respect to the ReLU operation.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wr2vCO-jFlxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Advanced B-cos networks**\n",
        "\n",
        "To test the generality of this approach, we evaluate how integrating the B-cos transform into commonly used DNN architectures affects their classification performance and interpretability.\n",
        "\n",
        "In order to \"convert\" such models to B-cos networks we proceed as follows:\n",
        "\n",
        "*   First, every convolutional kernel / fully connected layer is replaced by the corresponding B-cos version with two MaxOut units.\n",
        "\n",
        "*   Secondly, any other non-linearities (e.g., ReLU, MaxPool, etc.), as well as any batch norm layers are removed to maintain the alignment pressure and to ensure that the model can be summarised via a single linear transform.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**Models**\n",
        "\n",
        "For the experiments, we rely on the publicly available implementations of the VGG-11, ResNet-34, InceptionNet (v3), and DenseNet-121 model architectures. We adapt those architectures to B-cos networks as described before.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pP6CYQa2EhTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet-34"
      ],
      "metadata": {
        "id": "HNdvKjU7KL-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = {'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth'}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, dilation: int = 1) -> BcosConv2d:\n",
        "    return BcosConv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1)  # 3x3 convolution with padding\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> BcosConv2d:\n",
        "    return BcosConv2d(in_planes, out_planes, kernel_size=1, stride=stride)  # 1x1 convolution without padding\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        short_cat=False\n",
        "    ) -> None:\n",
        "\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "        self.stride = stride\n",
        "\n",
        "        self.short_cat = BcosConv2d(2 * planes, planes) if short_cat else None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        if self.short_cat is not None:\n",
        "            out = self.short_cat(torch.cat([out, identity], dim=1))\n",
        "\n",
        "        else:\n",
        "            out += identity\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        short_cat = False\n",
        "    ) -> None:\n",
        "\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.conv2 = conv3x3(width, width, stride)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "        self.stride = stride\n",
        "\n",
        "        self.short_cat = BcosConv2d(2 * planes * self.expansion, planes * self.expansion) if short_cat else None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        if self.short_cat is not None:\n",
        "            out = self.short_cat(torch.cat([out, identity], dim=1))\n",
        "\n",
        "        else:\n",
        "            out += identity\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 10,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        short_cat: bool = False\n",
        "    ) -> None:\n",
        "\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "\n",
        "        self.groups = groups\n",
        "        self.short_cat = short_cat\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        self.conv1 = BcosConv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3)\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
        "\n",
        "        self.fc = BcosConv2d(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def get_features(self, x):\n",
        "        return self.get_sequential_model()[:-1](x)\n",
        "\n",
        "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int, stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
        "\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(conv1x1(self.inplanes, planes * block.expansion, stride))\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer, self.short_cat))\n",
        "        self.inplanes = planes * block.expansion\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def get_sequential_model(self):\n",
        "\n",
        "        model = nn.Sequential(\n",
        "                              self.conv1,\n",
        "                              self.avgpool,\n",
        "                              *[m for m in self.layer1],\n",
        "                              *[m for m in self.layer2],\n",
        "                              *[m for m in self.layer3],\n",
        "                              *[m for m in self.layer4],\n",
        "                              self.fc\n",
        "                              )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def get_layer_idx(self, idx):\n",
        "        return int(np.ceil(len(self.get_sequential_model())*idx/10))\n",
        "\n",
        "    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    arch: str,\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\n",
        "    layers: List[int],\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    **kwargs: Any,\n",
        ") -> ResNet:\n",
        "\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_url[arch], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    \"\"\"ResNet-34 model from \"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet.\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)"
      ],
      "metadata": {
        "id": "B3wWx9Tih3cw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG-11"
      ],
      "metadata": {
        "id": "JxR7s6PeKSvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = {'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth'}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features: nn.Module, num_classes: int = 10, init_weights: bool = True) -> None:\n",
        "        super(VGG, self).__init__()\n",
        "\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "                                        BcosConv2d(512, 4096, kernel_size=7, padding=3, scale_fact=1000),\n",
        "                                        BcosConv2d(4096, 4096, scale_fact=1000),\n",
        "                                        BcosConv2d(4096, num_classes, scale_fact=1000),\n",
        "                                    )\n",
        "\n",
        "        if init_weights:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        return x\n",
        "\n",
        "    def get_features(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "    def get_sequential_model(self):\n",
        "        model = nn.Sequential(\n",
        "                              *[m for m in self.features], self.classifier\n",
        "                              )\n",
        "        return model\n",
        "\n",
        "    def get_layer_idx(self, idx):\n",
        "        return int(np.ceil(len(self.get_sequential_model())*idx/10))\n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        for m in self.modules():\n",
        "\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def make_layers(cfg: List[Union[str, int]], batch_norm: bool = False, in_channels: int = 3, no_pool = False) -> nn.Sequential:\n",
        "\n",
        "    layers: List[nn.Module] = []\n",
        "    in_channels = in_channels\n",
        "\n",
        "    new_config = []\n",
        "    for idx, entry in enumerate(cfg):\n",
        "        new_config.append([entry, 1])\n",
        "        if entry == \"M\" and no_pool:\n",
        "            new_config[idx-1][1] = 2\n",
        "\n",
        "    for v, stride in new_config:\n",
        "        if v == 'M':\n",
        "            if no_pool:\n",
        "                continue\n",
        "            layers += [nn.AvgPool2d(kernel_size=2, stride=2)]\n",
        "\n",
        "        else:\n",
        "            v = cast(int, v)\n",
        "            conv2d = BcosConv2d(in_channels, v, kernel_size=3, padding=1, stride=stride, scale_fact=1000)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d]\n",
        "            in_channels = v\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfgs: Dict[str, List[Union[str, int]]] = {'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']}\n",
        "\n",
        "\n",
        "def _vgg(arch: str, cfg: str, batch_norm: bool, pretrained: bool, progress: bool, in_channels=3, no_pool=False, **kwargs: Any) -> VGG:\n",
        "\n",
        "    if pretrained:\n",
        "        kwargs['init_weights'] = False\n",
        "\n",
        "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm, in_channels=in_channels, no_pool=no_pool), **kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_url[arch], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def vgg11(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> VGG:\n",
        "\n",
        "    \"\"\"VGG 11-layer model (configuration \"A\") from \"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return _vgg('vgg11', 'A', False, pretrained, progress, **kwargs)"
      ],
      "metadata": {
        "id": "J_HnytDv7gTC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNet-121"
      ],
      "metadata": {
        "id": "2T4v93r4LGMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['DenseNet', 'densenet121']\n",
        "\n",
        "model_url = {'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth'}\n",
        "\n",
        "\n",
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_input_features: int,\n",
        "        growth_rate: int,\n",
        "        bn_size: int,\n",
        "        drop_rate: float,\n",
        "        memory_efficient: bool = False,\n",
        "        b_exp=2,\n",
        "        max_out=2\n",
        "    ) -> None:\n",
        "        super(_DenseLayer, self).__init__()\n",
        "\n",
        "        self.conv1 = BcosConv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, max_out=max_out, padding=0, b=b_exp)\n",
        "\n",
        "        self.conv2 = BcosConv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, max_out=max_out, padding=1, b=b_exp)\n",
        "\n",
        "        self.drop_rate = float(drop_rate)\n",
        "        self.memory_efficient = memory_efficient\n",
        "\n",
        "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(concated_features)\n",
        "        return bottleneck_output\n",
        "\n",
        "    def any_requires_grad(self, input: List[Tensor]) -> bool:\n",
        "        for tensor in input:\n",
        "            if tensor.requires_grad:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def call_checkpoint_bottleneck(self, input: List[Tensor]) -> Tensor:\n",
        "\n",
        "        def closure(*inputs):\n",
        "            return self.bn_function(inputs)\n",
        "\n",
        "        return cp.checkpoint(closure, *input)\n",
        "\n",
        "    @torch.jit._overload_method\n",
        "    def forward(self, input: List[Tensor]) -> Tensor:\n",
        "        pass\n",
        "\n",
        "    @torch.jit._overload_method\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        pass\n",
        "\n",
        "    # torchscript does not yet support *args, so we overload method\n",
        "    # allowing it to take either a List[Tensor] or single Tensor\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "\n",
        "        if isinstance(input, Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
        "            if torch.jit.is_scripting():\n",
        "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
        "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
        "        else:\n",
        "            bottleneck_output = self.bn_function(prev_features)\n",
        "\n",
        "        new_features = self.conv2(bottleneck_output)\n",
        "\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "\n",
        "        return new_features\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int,\n",
        "        num_input_features: int,\n",
        "        bn_size: int,\n",
        "        growth_rate: int,\n",
        "        drop_rate: float,\n",
        "        memory_efficient: bool = False,\n",
        "        b_exp = 2,\n",
        "        max_out = 2\n",
        "    ) -> None:\n",
        "        super(_DenseBlock, self).__init__()\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                                num_input_features + i * growth_rate,\n",
        "                                growth_rate=growth_rate,\n",
        "                                bn_size=bn_size,\n",
        "                                drop_rate=drop_rate,\n",
        "                                memory_efficient=memory_efficient,\n",
        "                                b_exp=b_exp,\n",
        "                                max_out=max_out\n",
        "                                )\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features: Tensor) -> Tensor:\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features: int, num_output_features: int, b_exp=2, max_out=2) -> None:\n",
        "        super(_Transition, self).__init__()\n",
        "        self.conv = BcosConv2d(num_input_features, num_output_features, kernel_size=1, stride=1, padding=0, b=b_exp, max_out=max_out)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    r\"\"\"Densenet-BC model class, based on\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        growth_rate: int = 32,\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n",
        "        num_init_features: int = 64,\n",
        "        bn_size: int = 4,\n",
        "        drop_rate: float = 0,\n",
        "        num_classes: int = 10,\n",
        "        memory_efficient: bool = False,\n",
        "        b_exp = 2,\n",
        "        max_out = 2\n",
        "    ) -> None:\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "                                                  ('conv0', BcosConv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, b=b_exp, max_out=max_out)),\n",
        "                                                  ('pool0', nn.AvgPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "                                                  ]))\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "\n",
        "            block = _DenseBlock(\n",
        "                                  num_layers=num_layers,\n",
        "                                  num_input_features=num_features,\n",
        "                                  bn_size=bn_size,\n",
        "                                  growth_rate=growth_rate,\n",
        "                                  drop_rate=drop_rate,\n",
        "                                  memory_efficient=memory_efficient,\n",
        "                                  b_exp=b_exp,\n",
        "                                  max_out=max_out\n",
        "                                )\n",
        "\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2, b_exp=b_exp, max_out=max_out)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        self.classifier = BcosConv2d(num_features, num_classes, kernel_size=1, stride=1, padding=0, b=b_exp, max_out=max_out)\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def get_features(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "    def get_sequential_model(self):\n",
        "        model = nn.Sequential(*[m for m in self.features], self.classifier)\n",
        "        return model\n",
        "\n",
        "    def get_layer_idx(self, idx):\n",
        "        return idx\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        features = self.features(x)\n",
        "        out = self.classifier(features)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        return out\n",
        "\n",
        "\n",
        "def _load_state_dict(model: nn.Module, model_url: str, progress: bool) -> None:\n",
        "    # '.'s are no longer allowed in module names, but previous _DenseLayer\n",
        "    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "    # They are also in the checkpoints in model_urls. This pattern is used\n",
        "    # to find such keys.\n",
        "\n",
        "    pattern = re.compile(r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "\n",
        "    state_dict = load_state_dict_from_url(model_url, progress=progress)\n",
        "    for key in list(state_dict.keys()):\n",
        "        res = pattern.match(key)\n",
        "        if res:\n",
        "            new_key = res.group(1) + res.group(2)\n",
        "            state_dict[new_key] = state_dict[key]\n",
        "            del state_dict[key]\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def _densenet(\n",
        "    arch: str,\n",
        "    growth_rate: int,\n",
        "    block_config: Tuple[int, int, int, int],\n",
        "    num_init_features: int,\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    **kwargs: Any\n",
        ") -> DenseNet:\n",
        "\n",
        "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        _load_state_dict(model, model_url[arch], progress)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet121(pretrained: bool = False, progress: bool = True, num_init_features=64, growth_rate=32, **kwargs: Any) -> DenseNet:\n",
        "\n",
        "    r\"\"\"Densenet-121 model from \"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet.\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr.\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient, but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\n",
        "    \"\"\"\n",
        "\n",
        "    return _densenet('densenet121', growth_rate, (6, 12, 24, 16), num_init_features, pretrained, progress, **kwargs)"
      ],
      "metadata": {
        "id": "p5VV-BxWHDzf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### InceptionNet (v3)"
      ],
      "metadata": {
        "id": "vNJgU7lcKco_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['Inception3','InceptionOutputs', '_InceptionOutputs', 'inception_v3']\n",
        "\n",
        "model_url = {'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'}\n",
        "\n",
        "InceptionOutputs = namedtuple('InceptionOutputs', ['logits', 'aux_logits'])\n",
        "InceptionOutputs.__annotations__ = {'logits': Tensor, 'aux_logits': Optional[Tensor]}\n",
        "\n",
        "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
        "# _InceptionOutputs set here for backwards compat\n",
        "_InceptionOutputs = InceptionOutputs\n",
        "\n",
        "\n",
        "class Inception3(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 10,\n",
        "        aux_logits: bool = False,\n",
        "        transform_input: bool = False,\n",
        "        inception_blocks: Optional[List[Callable[..., nn.Module]]] = None,\n",
        "        init_weights: Optional[bool] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(Inception3, self).__init__()\n",
        "\n",
        "        if inception_blocks is None:\n",
        "            inception_blocks = [BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE, InceptionAux]\n",
        "\n",
        "        if init_weights is None:\n",
        "            warnings.warn('The default weight initialization of inception_v3 will be changed in future releases of '\n",
        "                          'torchvision. If you wish to keep the old behavior (which leads to long initialization times'\n",
        "                          ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
        "            init_weights = True\n",
        "\n",
        "        assert len(inception_blocks) == 7\n",
        "        conv_block = inception_blocks[0]\n",
        "        inception_a = inception_blocks[1]\n",
        "        inception_b = inception_blocks[2]\n",
        "        inception_c = inception_blocks[3]\n",
        "        inception_d = inception_blocks[4]\n",
        "        inception_e = inception_blocks[5]\n",
        "        inception_aux = inception_blocks[6]\n",
        "\n",
        "        self.aux_logits = aux_logits\n",
        "        self.transform_input = transform_input\n",
        "\n",
        "        self.Conv2d_1a_3x3 = conv_block(3, 32, kernel_size=3, stride=2)\n",
        "        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n",
        "        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n",
        "        self.avgpool1 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n",
        "        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n",
        "        self.avgpool2 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.Mixed_5b = inception_a(192, pool_features=32)\n",
        "        self.Mixed_5c = inception_a(256, pool_features=64)\n",
        "        self.Mixed_5d = inception_a(288, pool_features=64)\n",
        "\n",
        "        self.Mixed_6a = inception_b(288)\n",
        "        self.Mixed_6b = inception_c(768, channels_7x7=128)\n",
        "        self.Mixed_6c = inception_c(768, channels_7x7=160)\n",
        "        self.Mixed_6d = inception_c(768, channels_7x7=160)\n",
        "        self.Mixed_6e = inception_c(768, channels_7x7=192)\n",
        "\n",
        "        self.AuxLogits: Optional[nn.Module] = None\n",
        "        if aux_logits:\n",
        "            self.AuxLogits = inception_aux(768, num_classes)\n",
        "\n",
        "        self.Mixed_7a = inception_d(768)\n",
        "        self.Mixed_7b = inception_e(1280)\n",
        "        self.Mixed_7c = inception_e(2048)\n",
        "\n",
        "        self.fc = BcosConv2d(2048, num_classes, kernel_size=1, stride=1, padding=0, scale_fact=200)\n",
        "\n",
        "        self.debug = False\n",
        "\n",
        "        if init_weights:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                    import scipy.stats as stats\n",
        "                    stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
        "                    X = stats.truncnorm(-2, 2, scale=stddev)\n",
        "                    values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
        "                    values = values.view(m.weight.size())\n",
        "                    with torch.no_grad():\n",
        "                        m.weight.copy_(values)\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def get_features(self, x):\n",
        "        return self.get_sequential_model()[:-1](x)\n",
        "\n",
        "    def _transform_input(self, x: Tensor) -> Tensor:\n",
        "        return x\n",
        "\n",
        "    def get_sequential_model(self):\n",
        "        \"\"\" For evaluation purposes only, to extract layers at roughly the same relative network depth between different models. \"\"\"\n",
        "        model = nn.Sequential(\n",
        "                                self.Conv2d_1a_3x3,\n",
        "                                self.Conv2d_2a_3x3,\n",
        "                                self.Conv2d_2b_3x3,\n",
        "                                self.avgpool1,\n",
        "                                self.Conv2d_3b_1x1,\n",
        "                                self.Conv2d_4a_3x3,\n",
        "                                self.avgpool2,\n",
        "                                self.Mixed_5b,\n",
        "                                self.Mixed_5c,\n",
        "                                self.Mixed_5d,\n",
        "                                self.Mixed_6a,\n",
        "                                self.Mixed_6b,\n",
        "                                self.Mixed_6c,\n",
        "                                self.Mixed_6d,\n",
        "                                self.Mixed_6e,\n",
        "                                self.Mixed_7a,\n",
        "                                self.Mixed_7b,\n",
        "                                self.Mixed_7c,\n",
        "                                self.fc\n",
        "                              )\n",
        "        return model\n",
        "\n",
        "    def get_layer_idx(self, idx):\n",
        "        \"\"\" For evaluation purposes only, to extract layers at roughly the same relative network depth between different models. \"\"\"\n",
        "        return int(np.ceil(len(self.get_sequential_model())*idx/10))\n",
        "\n",
        "    def print(self, layer_name, x):\n",
        "        if self.debug:\n",
        "            print(layer_name, x.shape)\n",
        "\n",
        "    def _forward(self, x: Tensor):\n",
        "        # N x 3 x 299 x 299\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # N x 32 x 149 x 149\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # N x 32 x 147 x 147\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # N x 64 x 147 x 147\n",
        "        x = self.avgpool1(x)\n",
        "        # N x 64 x 73 x 73\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # N x 80 x 73 x 73\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # N x 192 x 71 x 71\n",
        "        x = self.avgpool2(x)\n",
        "        # N x 192 x 35 x 35\n",
        "        x = self.Mixed_5b(x)\n",
        "        # N x 256 x 35 x 35\n",
        "        x = self.Mixed_5c(x)\n",
        "        # N x 288 x 35 x 35\n",
        "        x = self.Mixed_5d(x)\n",
        "        # N x 288 x 35 x 35\n",
        "        x = self.Mixed_6a(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6b(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6c(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6d(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6e(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        aux: Optional[Tensor] = None\n",
        "        if self.AuxLogits is not None:\n",
        "            if self.training:\n",
        "                self.aux_out = self.AuxLogits(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_7a(x)\n",
        "        # N x 1280 x 8 x 8\n",
        "        x = self.Mixed_7b(x)\n",
        "        # N x 2048 x 8 x 8\n",
        "        x = self.Mixed_7c(x)\n",
        "        # N x 2048 x 1 x 1\n",
        "        x = self.fc(x)\n",
        "        # N x 1000 (num_classes)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        return x\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def eager_outputs(self, x: Tensor, aux: Optional[Tensor]):\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> InceptionOutputs:\n",
        "        x = self._transform_input(x)\n",
        "        x = self._forward(x)\n",
        "        aux_defined = self.training and self.aux_logits\n",
        "        if torch.jit.is_scripting():\n",
        "            if not aux_defined:\n",
        "                warnings.warn(\"Scripted Inception3 always returns Inception3 Tuple\")\n",
        "            return InceptionOutputs(x, None)\n",
        "        else:\n",
        "            return self.eager_outputs(x, None)\n",
        "\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        pool_features: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionA, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "\n",
        "        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "\n",
        "        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n",
        "        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = self.pool(x)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionB(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionB, self).__init__()\n",
        "\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "\n",
        "        self.branch3x3 = conv_block(in_channels, 384, kernel_size=1, stride=2)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=1, stride=2)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = self.pool(x)\n",
        "\n",
        "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionC(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        channels_7x7: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionC, self).__init__()\n",
        "\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "        c7 = channels_7x7\n",
        "        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n",
        "        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n",
        "        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        branch_pool = self.pool(x)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionD(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionD, self).__init__()\n",
        "\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "\n",
        "        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "        self.branch3x3_2 = conv_block(192, 320, kernel_size=1, stride=2)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=1, stride=2)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = self.branch3x3_2(branch3x3)\n",
        "\n",
        "        branch7x7x3 = self.branch7x7x3_1(x)\n",
        "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
        "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
        "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
        "\n",
        "        branch_pool = self.pool(x)\n",
        "\n",
        "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionE(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionE, self).__init__()\n",
        "\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n",
        "\n",
        "        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n",
        "        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "                      self.branch3x3_2a(branch3x3),\n",
        "                      self.branch3x3_2b(branch3x3),\n",
        "                    ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "                          self.branch3x3dbl_3a(branch3x3dbl),\n",
        "                          self.branch3x3dbl_3b(branch3x3dbl),\n",
        "                       ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        branch_pool = self.pool(x)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionAux(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_classes: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionAux, self).__init__()\n",
        "\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "\n",
        "        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
        "\n",
        "        self.conv1 = conv_block(128, 768, kernel_size=5)\n",
        "        self.conv1.stddev = 0.01\n",
        "\n",
        "        self.fc = BcosConv2d(768, num_classes, kernel_size=1, stride=1, padding=0, scale_fact=200)\n",
        "        self.fc.stddev = 0.001\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.pool(x)\n",
        "        # N x 768 x 5 x 5\n",
        "        x = self.conv0(x)\n",
        "        # N x 128 x 5 x 5\n",
        "        x = self.conv1(x)\n",
        "        x = self.fc(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))[..., 0, 0]\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "\n",
        "        super(BasicConv2d, self).__init__()\n",
        "\n",
        "        self.conv = BcosConv2d(in_channels, out_channels, scale_fact=200, **kwargs)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "def inception_v3(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> \"Inception3\":\n",
        "\n",
        "    r\"\"\"Inception v3 model architecture from \"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>_.\n",
        "\n",
        "    .. note:: **Important**: In contrast to the other models the inception_v3 expects tensors with a size of N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet.\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr.\n",
        "        aux_logits (bool): If True, add an auxiliary branch that can improve training. Default: *True*.\n",
        "        transform_input (bool): If True, preprocesses the input according to the method with which it was trained on ImageNet. Default: *False*.\n",
        "    \"\"\"\n",
        "\n",
        "    if pretrained:\n",
        "\n",
        "        if 'transform_input' not in kwargs:\n",
        "            kwargs['transform_input'] = True\n",
        "\n",
        "        if 'aux_logits' in kwargs:\n",
        "            original_aux_logits = kwargs['aux_logits']\n",
        "            kwargs['aux_logits'] = True\n",
        "        else:\n",
        "            original_aux_logits = True\n",
        "\n",
        "        kwargs['init_weights'] = False  # We are loading weights from a pretrained model.\n",
        "\n",
        "        model = Inception3(**kwargs)\n",
        "\n",
        "        state_dict = load_state_dict_from_url(model_url['inception_v3_google'], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "        if not original_aux_logits:\n",
        "            model.aux_logits = False\n",
        "            model.AuxLogits = None\n",
        "        return model\n",
        "\n",
        "    return Inception3(**kwargs)"
      ],
      "metadata": {
        "id": "qbXLkVsxGzQB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "3_CTvJcVWNO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Training**\n",
        "\n",
        "We trained our models for 5 epochs with Adam, an initial learning rate of 0.001 and a batch size of 64.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IaADtvEoWX9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getArgs():\n",
        "    parser = argparse.ArgumentParser(description=\"BCOS_TRAINING\")\n",
        "    parser.add_argument('-f')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    args.batch_size = 64\n",
        "    args.dataset = 'CIFAR10'\n",
        "    args.epochs = 15\n",
        "    args.learning_rate = 0.001\n",
        "    args.model_name = 'resnet34'\n",
        "    args.model = resnet34()\n",
        "\n",
        "    args.cifar10Path = '/var/datasets/CIFAR10'\n",
        "\n",
        "    args.save_losses = '/content/'\n",
        "    args.save_ckpt = '/content/ckpt/'\n",
        "\n",
        "    args.load_ckpt = f'/content/ckpt/Epoch_{args.epochs}.pt'\n",
        "    args.save_results_path = '/content/'\n",
        "\n",
        "    return args"
      ],
      "metadata": {
        "id": "SbkVxDB1grJk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Optimising B-cos networks for classification**\n",
        "\n",
        "*   First, note that the output of each neuron is bounded.\n",
        "Since the output of a B-cos network is computed as a sequence of such bounded transforms, the output of the network as a whole is also bounded.\n",
        "\n",
        "*   Secondly, note that a B-cos network as a whole can only achieve its upper bound for a given input if the units in each layer achieve their upper bound.\n",
        "The individual units, in turn, can only achieve their maxima by aligning with their inputs.\n",
        "Hence, optimising a B-cos network to maximise its output over a set of inputs will optimise the model weights to align with those inputs.\n",
        "\n",
        "In order to take advantage of this when optimising for classification, we train the B-cos networks with the **Binary Cross Entropy** (**BCE**) loss:\n",
        "\n",
        "      L(xᵢ, yᵢ) = BCE(σ(f(xᵢ; θ) + b), yᵢ)`,\n",
        "for input xᵢ and its corresponding one-hot encoded class label yᵢ.\n",
        "Here, σ denotes the sigmoid function, b a bias, and θ the model parameters.\n",
        "\n",
        "We choose the BCE loss because it directly entails output maximisation. Specifically, in order to reduce the BCE loss, the network is optimised to maximise the (negative) class logit for the correct (incorrect) classes.\n",
        "\n",
        "Finally, note that increasing B allows to specifically reduce the output of badly aligned weights in each layer.\n",
        "This will decrease the layer’s output strength and thus the output of the network as a whole for badly aligned weights, which increases the alignment pressure during optimisation (thus, **higher B** → **higher alignment**).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fYYuX1KHWTw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        self.loader = loadData(args)\n",
        "        self.model = args.model\n",
        "        self.create_paths(args.save_ckpt, args.save_losses)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_paths(ckpt_path, losses_path):\n",
        "\n",
        "        if not os.path.exists(ckpt_path):\n",
        "            os.makedirs(ckpt_path)\n",
        "\n",
        "        if not os.path.exists(losses_path):\n",
        "            os.makedirs(losses_path)\n",
        "\n",
        "    def training(self, args):\n",
        "\n",
        "        # LOADING DATA #\n",
        "        train_dataloader, val_dataloader = self.loader.getDataLoader()\n",
        "\n",
        "        # TRAINING PARAMETERS #\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "        all_train_loss = []\n",
        "        all_val_loss = []\n",
        "\n",
        "        # print(self.model)\n",
        "\n",
        "        #############################\n",
        "        #       TRAINING LOOP       #\n",
        "        #############################\n",
        "\n",
        "        print(f'\\nSTARTING TRAINING WITH {args.model_name} FOR {args.dataset}')\n",
        "\n",
        "        for epoch in range(args.epochs):\n",
        "            epoch_train_loss = []\n",
        "            epoch_val_loss = []\n",
        "\n",
        "            self.model.train()\n",
        "            for imgs, labels in tqdm(train_dataloader):\n",
        "                labels = F.one_hot(labels, num_classes=10)\n",
        "\n",
        "                logits = self.model(imgs)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                loss = criterion(logits, labels.float())\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_train_loss.append(loss.detach().numpy())\n",
        "\n",
        "            self.model.eval()\n",
        "            for imgs, labels in tqdm(val_dataloader):\n",
        "                labels = F.one_hot(labels, num_classes=10)\n",
        "\n",
        "                logits = self.model(imgs)\n",
        "\n",
        "                loss = criterion(logits, labels.float())\n",
        "\n",
        "                epoch_val_loss.append(loss.detach().numpy())\n",
        "\n",
        "            train_loss, val_loss = np.mean(epoch_train_loss), np.mean(epoch_val_loss)\n",
        "\n",
        "            all_train_loss.append(train_loss)\n",
        "            all_val_loss.append(val_loss)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                np.save(os.path.join(args.save_losses, 'train_loss.npy'), all_train_loss)\n",
        "                np.save(os.path.join(args.save_losses, 'val_loss.npy'), all_val_loss)\n",
        "                torch.save(self.model.state_dict(), os.path.join(args.save_ckpt, f'Epoch_{args.epochs}.pt'))\n",
        "\n",
        "        print(f'\\n--- FINISHED {args.model_name} TRAINING ---')\n",
        "\n",
        "\n",
        "args = getArgs()\n",
        "\n",
        "# TRAINING\n",
        "Trainer(args).training(args)"
      ],
      "metadata": {
        "id": "Xej-ZNpO3I1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "ILJymRTnYXKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Evaluating explanations**\n",
        "\n",
        "We evaluate the B-cos networks across all models to investigate which one provides the best explanation.\n",
        "\n",
        "This makes it possible to compare explanations between different models and to evaluate the explainability gain achieved by converting conventional models to B-cos networks.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0HnBFe91qNdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getResults(y_true, y_pred):\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    dict_metrics = {'accuracy' : [accuracy],\n",
        "                    'f1-score' : [f1],\n",
        "                    'precision' : [precision],\n",
        "                    'recall' : [recall]}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dict_metrics)\n",
        "\n",
        "    print('\\n\\n-- CONFUSION MATRIX --\\n')\n",
        "    print(conf_matrix)\n",
        "\n",
        "    print('\\n-- CLASSIFICATION METRICS --\\n')\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    return dict_metrics\n",
        "\n",
        "\n",
        "# From the original implementation\n",
        "def getExplanationImage(img, grads, smooth=15, alpha_percent=99.5):\n",
        "\n",
        "    contribution = (img*grads).sum(0, keepdim=True)\n",
        "    # print('contribution', contribution.shape)\n",
        "\n",
        "    rgb_grad = (grads / (grads.abs().max(0, keepdim=True)[0] + 1e-12))\n",
        "    rgb_grad = rgb_grad.clamp(0).numpy()\n",
        "    # print('rgb_grad', rgb_grad.shape)\n",
        "\n",
        "    alpha = (grads.norm(p=2, dim=0, keepdim=True))\n",
        "    # print('alpha', alpha.shape)\n",
        "\n",
        "    # Only show positive contributions\n",
        "    alpha = torch.where(contribution < 0, torch.zeros_like(alpha) + 1e-12, alpha)\n",
        "\n",
        "    if smooth:\n",
        "        alpha = F.avg_pool2d(alpha, smooth, stride=1, padding=(smooth-1)//2)\n",
        "\n",
        "    alpha = alpha.numpy()\n",
        "    alpha = (alpha / np.percentile(alpha, alpha_percent)).clip(0, 1)\n",
        "    # print('alpha', alpha.shape)\n",
        "\n",
        "    rgb_grad = np.concatenate([rgb_grad, alpha], axis=0)[1:]\n",
        "    # print('rgb_grad', rgb_grad.shape)\n",
        "\n",
        "    # Reshaping to [H, W, C]\n",
        "    grad_image = rgb_grad.transpose((1, 2, 0))\n",
        "    grad_image = torch.tensor(grad_image)\n",
        "    grad_image = grad_image.permute(2, 0, 1)\n",
        "    # print('grad_image', grad_image.shape)\n",
        "\n",
        "    return grad_image\n",
        "\n",
        "\n",
        "class Inference:\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        self.loader = loadData(args)\n",
        "        self.model = self.getModels(args)\n",
        "\n",
        "    def getModels(self, args):\n",
        "\n",
        "        # LOADING MODEL\n",
        "        model = args.model\n",
        "\n",
        "        if args.load_ckpt is not None:\n",
        "            path = args.load_ckpt.split('ckpt/')[-1]\n",
        "            print(f'LOADING MODEL: {path}\\n')\n",
        "            model.load_state_dict(torch.load(args.load_ckpt), strict=False)\n",
        "        else:\n",
        "            print('LOADING MODEL: no checkpoint -> initializing randomly\\n')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def evaluateMetrics(self, args):\n",
        "\n",
        "        # LOADING DATA #\n",
        "        train_dataloader, val_dataloader = self.loader.getDataLoader()\n",
        "\n",
        "        # TRAINING PARAMETERS #\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        #############################\n",
        "        #       INFERENCE LOOP      #\n",
        "        #############################\n",
        "\n",
        "        print(f'\\nSTARTING CLASSIFICATION WITH {args.model_name} FOR {args.dataset}')\n",
        "\n",
        "        all_pred = []\n",
        "        all_labels = []\n",
        "\n",
        "        self.model.eval()\n",
        "        for imgs, labels in tqdm(val_dataloader):\n",
        "\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            logits = self.model(imgs)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            max_values, pred = torch.max(probs, dim=-1)\n",
        "\n",
        "            pred, labels = list(pred.detach().numpy()), list(labels.detach().numpy())\n",
        "            all_pred = np.concatenate((all_pred, pred))\n",
        "            all_labels = np.concatenate((all_labels, labels))\n",
        "\n",
        "        all_pred = torch.tensor(all_pred)\n",
        "        all_labels = torch.tensor(all_labels)\n",
        "\n",
        "        getResults(all_labels, all_pred)\n",
        "\n",
        "    def getExplanations(self, args):\n",
        "\n",
        "        # LOADING DATA #\n",
        "        train_dataloader, val_dataloader = self.loader.getDataLoader()\n",
        "\n",
        "        # TRAINING PARAMETERS #\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        #############################\n",
        "        #       INFERENCE LOOP      #\n",
        "        #############################\n",
        "\n",
        "        print(f'\\nSTARTING EXPLANATIONS GENERATION WITH {args.model_name} FOR {args.dataset}')\n",
        "\n",
        "        ori_images = []\n",
        "        exp_images = []\n",
        "\n",
        "        self.model.eval()\n",
        "        for imgs, labels in tqdm(val_dataloader):\n",
        "\n",
        "            # from the original implementation\n",
        "            imgs = imgs.requires_grad_(True)\n",
        "\n",
        "            logits = self.model(imgs)\n",
        "            max_logit = self.model(imgs).max()\n",
        "\n",
        "            imgs.grad = None\n",
        "            self.model.zero_grad()\n",
        "            max_logit.backward()\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            max_values, max_indices = torch.max(logits, dim=-1)\n",
        "            max_probs, max_img_in_batch = torch.max(max_values, dim=0)\n",
        "\n",
        "            explanation = getExplanationImage(imgs[max_img_in_batch], imgs.grad[max_img_in_batch])\n",
        "\n",
        "            ori_images.append(imgs[max_img_in_batch])\n",
        "            exp_images.append(explanation)\n",
        "\n",
        "        ori_images = torch.stack(ori_images)\n",
        "        # print(ori_images.shape)\n",
        "\n",
        "        exp_images = torch.stack(exp_images)\n",
        "        # print(exp_images.shape)\n",
        "\n",
        "        real_fake_images = torch.cat((ori_images[:5], exp_images.add(1).mul(0.5)[:5]))\n",
        "        utils.save_image(real_fake_images, os.path.join(args.save_results_path, 'explanation_results.jpg'), nrow=5)\n",
        "\n",
        "        print(f'\\n\\n--- FINISHED {args.model_name} EXPLANATIONS GENERATION ---')\n",
        "\n",
        "\n",
        "args = getArgs()\n",
        "\n",
        "# Inference(args).evaluateMetrics(args)\n",
        "Inference(args).getExplanations(args)"
      ],
      "metadata": {
        "id": "wC_XkV8mQEuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
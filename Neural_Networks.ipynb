{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "czCd2JW8U7o7",
        "4zmncgl0VESD",
        "2T4v93r4LGMG",
        "vNJgU7lcKco_",
        "ILJymRTnYXKc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# B-cos Networks: Alignment is All We Need for Interpretability"
      ],
      "metadata": {
        "id": "aAf1mf4gKKup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In this notebook, we present a new direction for increasing the interpretability of Deep Neural Networks (DNNs) by proposing to replace the linear transforms in DNNs by the **B-cos transform**.\n",
        "\n",
        "The B-cos transform is designed to be compatible with existing architectures and\n",
        "we show that it can easily be integrated into common models such as *VGGs*, *ResNets*, *InceptionNets*, and *DenseNets*, whilst maintaining similar performance.\n",
        "\n",
        "The resulting explanations are of high visual quality and perform well under quantitative metrics for interpretability.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "A__EM-rZLrjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "czCd2JW8U7o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.checkpoint as cp\n",
        "import torchvision\n",
        "import warnings\n",
        "\n",
        "from collections import namedtuple, OrderedDict\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
        "from typing import Any, Callable, Dict, List, Optional, Type, Tuple, Union, cast\n",
        "from torch import Tensor\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision import utils\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "HC0N6AeQ3X2N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "4zmncgl0VESD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Datasets**\n",
        "\n",
        "We evaluate the accuracy of several B-cos networks on the *CIFAR-10* dataset.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HLFBwfcIldQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class loadData:\n",
        "    def __init__(self, args):\n",
        "\n",
        "        self.batch_size = args.batch_size\n",
        "\n",
        "        self.dataPath = args.cifar10Path\n",
        "        self.create_paths(self.dataPath)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_paths(path):\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "    def getDataLoader(self):\n",
        "\n",
        "        cifar10_transforms = transforms.Compose([\n",
        "                                                  transforms.RandomCrop(32, padding=4),\n",
        "                                                  transforms.RandomHorizontalFlip(),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "                                                 ])\n",
        "\n",
        "        train_loader = DataLoader(torchvision.datasets.CIFAR10(self.dataPath, train=True, download=True, transform=cifar10_transforms), batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        val_loader = DataLoader(torchvision.datasets.CIFAR10(self.dataPath, train=False, download=True, transform=cifar10_transforms), batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        return train_loader, val_loader"
      ],
      "metadata": {
        "id": "34yL-7RwBbUZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "BotNBV_TVjSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**The B-cos transform**\n",
        "\n",
        "Typically, the individual neurons in a DNN compute the dot product between their weights **w** and an input **x**:\n",
        "\n",
        "     f(x; w) = wᵀ x = ||w|| ||x|| c(x, w) with c(x, w) = cos(∠(x, w)).\n",
        "\n",
        "Here, `∠(x, w)` returns the angle between the vectors **x** and **w**.\n",
        "\n",
        "In this work, we seek to improve the interpretability of DNNs by promoting weight-input alignment during optimisation.\n",
        "\n",
        "To achieve this, we propose the ***B-cos transform***:\n",
        "\n",
        "     B-cos(x; w) = ||ŵ|| ||x|| |c(x, ŵ)|ᴮ × sgn (c(x, ŵ)).`\n",
        "\n",
        "Here, *B* is a hyperparameter, the hat-operator scales **ŵ** to unit norm, and `sgn` denotes the *sign* function.\n",
        "\n",
        "Note that this only introduces minor changes with respect to the first equation; e.g., for *B* = 1, the B-cos transform is equivalent to a linear transform with **ŵ**.\n",
        "\n",
        "These changes maintain an important property of the linear transform: similar to sequences of linear transforms, sequences of B-cos transforms can still be faithfully summarised by a single linear transform.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ivvyRwppUfr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NormedConv2d(nn.Conv2d):\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight_shape = self.weight.shape\n",
        "\n",
        "        w_hat = self.weight.view(weight_shape[0], -1)\n",
        "        w_hat = w_hat/(w_hat.norm(p=2, dim=1, keepdim=True))\n",
        "        w_hat = w_hat.view(weight_shape)\n",
        "\n",
        "        return F.conv2d(x, w_hat, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "class BcosConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, max_out=2, b=2, scale=None, scale_fact=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.NormedConv2d = NormedConv2d(in_channels, out_channels * max_out, kernel_size, stride, padding, dilation=1, groups=1, bias=False)\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.max_out = max_out\n",
        "        self.b = b\n",
        "\n",
        "        if scale is None:\n",
        "            kernel_size_scale = kernel_size if not isinstance(kernel_size, tuple) else np.sqrt(np.prod(kernel_size))\n",
        "            self.scale = (kernel_size_scale * np.sqrt(in_channels)) / scale_fact\n",
        "        else:\n",
        "            self.scale = scale\n",
        "\n",
        "        self.detach = False\n",
        "\n",
        "        self.kernel_size_power = kernel_size**2 if not isinstance(kernel_size, tuple) else np.prod(kernel_size)\n",
        "\n",
        "    def explanation_mode(self, detach=True):\n",
        "        self.detach = detach\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out_normed_conv2d = self.NormedConv2d(x)\n",
        "        batch_size, _, h, w = out_normed_conv2d.shape\n",
        "\n",
        "        # MaxOut computation.\n",
        "        if self.max_out > 1:\n",
        "            out_normed_conv2d = out_normed_conv2d.view(batch_size, -1, self.max_out, h, w)\n",
        "            out_normed_conv2d = out_normed_conv2d.max(dim=2, keepdim=False)[0]\n",
        "\n",
        "        # If B=1, no further calculation necessary.\n",
        "        if self.b == 1:\n",
        "            return out_normed_conv2d / self.scale\n",
        "\n",
        "        # Calculating the norm of input patches.\n",
        "        norm = (F.avg_pool2d((x**2).sum(1, keepdim=True), self.kernel_size, padding=self.padding, stride=self.stride) * self.kernel_size_power + 1e-6).sqrt_()\n",
        "\n",
        "        # Get absolute value of cos.\n",
        "        abs_cos = (out_normed_conv2d/norm).abs() + 1e-6\n",
        "\n",
        "        # In order to compute the explanations.\n",
        "        if self.detach:\n",
        "            abs_cos = abs_cos.detach()\n",
        "\n",
        "        # Additional factor of cos^(b-1).\n",
        "        out_normed_conv2d = out_normed_conv2d * abs_cos.pow(self.b-1)\n",
        "\n",
        "        return out_normed_conv2d / self.scale"
      ],
      "metadata": {
        "id": "X0lmkNE4MGjY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**B-cos networks**\n",
        "\n",
        "The B-cos transform is designed as a *drop-in* replacement of the linear transform, i.e., it can be used in exactly the same way.\n",
        "\n",
        "For example, a conventional fully connected multi-layer neural network f(**x**; θ) of L layers, is represented by:\n",
        "\n",
        "      `f(x; θ) = lL ◦ lL−1 ◦ ... ◦ l2 ◦ l1(x),`\n",
        "\n",
        "with lⱼ denoting layer j with parameters **w**ᵏⱼ for neuron k in layer j, and θ the collection of all model parameters.\n",
        "\n",
        "In such a model, each layer lⱼ typically computes:\n",
        "\n",
        "      `lⱼ(aⱼ; Wⱼ) = φ(Wⱼ aⱼ),`\n",
        "\n",
        "with aⱼ the input to layer j, φ a non-linear activation function (e.g., ReLU), and the row k of Wⱼ given by the weight vector **w**ᵏⱼ of the k-th neuron in that layer.\n",
        "\n",
        "A corresponding **B-cos network** f with layers lⱼ can be formulated in exactly the same way, with the only difference being that every dot product (here between rows of Wⱼ and the input aⱼ) is replaced by the B-cos transform.\n",
        "\n",
        "In matrix form, this equates to:\n",
        "\n",
        "       lⱼ(aⱼ; Wⱼ) = |c(aⱼ; Ŵⱼ)|^(B-1) × (Ŵⱼ aⱼ),`\n",
        "\n",
        "Here, the power, absolute value, and `×` operators are applied element-wise, `c(aⱼ; Ŵⱼ)` computes the cosine similarity between input aⱼ and the rows of Ŵⱼ, and the hat operator scales the rows of Ŵⱼ to unit norm.\n",
        "\n",
        "Finally, note that for *B* > 1 the layer transform lⱼ is non-linear.\n",
        "As a result, a non-linearity φ is not required for a B-cos network to model non-linear relationships.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**MaxOut to increase modelling capacity**\n",
        "\n",
        "As discussed, a deep B-cos network with *B* > 1 does not require a non-linearity between subsequent layers to model non-linear relationships.\n",
        "This, of course, does not mean that it could not benefit from it.\n",
        "\n",
        "In this work, we specifically explore the option of combining the B-cos\n",
        "transform with the MaxOut operation.\n",
        "In particular, we model every neuron in a B-cos network by 2 B-cos transforms of which the maximal activation is forwarded:\n",
        "\n",
        "       MaxOut(x) = max{B-cos(x; wᵢ)} with i∈{1,2},\n",
        "\n",
        "We noticed that networks with the MaxOut operation were much easier to optimise with respect to the ReLU operation.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wr2vCO-jFlxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Advanced B-cos networks**\n",
        "\n",
        "To test the generality of this approach, we evaluate how integrating the B-cos transform into commonly used DNN architectures affects their classification performance and interpretability.\n",
        "\n",
        "In order to \"convert\" such models to B-cos networks we proceed as follows:\n",
        "\n",
        "*   First, every convolutional kernel / fully connected layer is replaced by the corresponding B-cos version with two MaxOut units.\n",
        "\n",
        "*   Secondly, any other non-linearities (e.g., ReLU, MaxPool, etc.), as well as any batch norm layers are removed to maintain the alignment pressure and to ensure that the model can be summarised via a single linear transform.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "**Models**\n",
        "\n",
        "For the experiments, we rely on the publicly available implementations of the VGG-11, ResNet-34, InceptionNet (v3), and DenseNet-121 model architectures. We adapt those architectures to B-cos networks as described before.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pP6CYQa2EhTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet-34"
      ],
      "metadata": {
        "id": "HNdvKjU7KL-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1) -> BcosConv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return BcosConv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> BcosConv2d:\n",
        "    \"\"\"1x1 convolution without padding\"\"\"\n",
        "    return BcosConv2d(in_planes, out_planes, kernel_size=1, stride=stride)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(conv1x1(in_planes, self.expansion*planes, stride))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv1 = conv1x1(in_planes, planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(conv1x1(in_planes, self.expansion*planes, stride))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out += self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = BcosConv2d(3, self.in_planes, kernel_size=7, stride=2, padding=3)\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "        self.linear = BcosConv2d(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.avgpool(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.linear(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])"
      ],
      "metadata": {
        "id": "B3wWx9Tih3cw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG-11"
      ],
      "metadata": {
        "id": "JxR7s6PeKSvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "__all__ = [\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, features, num_classes=10):\n",
        "        super(VGG, self).__init__()\n",
        "\n",
        "        self.features = features\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "                                        BcosConv2d(512, 4096, kernel_size=7, padding=3, scale_fact=1000),\n",
        "                                        BcosConv2d(4096, 4096, scale_fact=1000),\n",
        "                                        BcosConv2d(4096, num_classes, scale_fact=1000),\n",
        "                                        )\n",
        "\n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "\n",
        "    for v in cfg:\n",
        "        if v != 'M':\n",
        "            conv2d = BcosConv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
        "          512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def vgg11():\n",
        "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
        "    return VGG(make_layers(cfg['A']))\n",
        "\n",
        "\n",
        "def vgg11_bn(channels=3):\n",
        "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['A'], batch_norm=True))\n",
        "\n",
        "\n",
        "def vgg13():\n",
        "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
        "    return VGG(make_layers(cfg['B']))\n",
        "\n",
        "\n",
        "def vgg13_bn():\n",
        "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
        "\n",
        "\n",
        "def vgg16():\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
        "    return VGG(make_layers(cfg['D']))\n",
        "\n",
        "\n",
        "def vgg16_bn(channels=3):\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
        "\n",
        "\n",
        "def vgg19():\n",
        "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
        "    return VGG(make_layers(cfg['E']))\n",
        "\n",
        "\n",
        "def vgg19_bn():\n",
        "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['E'], batch_norm=True))"
      ],
      "metadata": {
        "id": "J_HnytDv7gTC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNet-121"
      ],
      "metadata": {
        "id": "2T4v93r4LGMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['DenseNet', 'densenet121']\n",
        "\n",
        "model_url = {'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth'}\n",
        "\n",
        "\n",
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_input_features: int,\n",
        "        growth_rate: int,\n",
        "        bn_size: int,\n",
        "        drop_rate: float,\n",
        "        memory_efficient: bool = False,\n",
        "        b_exp=2,\n",
        "        max_out=2\n",
        "    ) -> None:\n",
        "        super(_DenseLayer, self).__init__()\n",
        "\n",
        "        self.conv1 = BcosConv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, max_out=max_out, padding=0, b=b_exp)\n",
        "\n",
        "        self.conv2 = BcosConv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, max_out=max_out, padding=1, b=b_exp)\n",
        "\n",
        "        self.drop_rate = float(drop_rate)\n",
        "        self.memory_efficient = memory_efficient\n",
        "\n",
        "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(concated_features)\n",
        "        return bottleneck_output\n",
        "\n",
        "    def any_requires_grad(self, input: List[Tensor]) -> bool:\n",
        "        for tensor in input:\n",
        "            if tensor.requires_grad:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def call_checkpoint_bottleneck(self, input: List[Tensor]) -> Tensor:\n",
        "\n",
        "        def closure(*inputs):\n",
        "            return self.bn_function(inputs)\n",
        "\n",
        "        return cp.checkpoint(closure, *input)\n",
        "\n",
        "    @torch.jit._overload_method\n",
        "    def forward(self, input: List[Tensor]) -> Tensor:\n",
        "        pass\n",
        "\n",
        "    @torch.jit._overload_method\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        pass\n",
        "\n",
        "    # torchscript does not yet support *args, so we overload method\n",
        "    # allowing it to take either a List[Tensor] or single Tensor\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "\n",
        "        if isinstance(input, Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
        "            if torch.jit.is_scripting():\n",
        "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
        "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
        "        else:\n",
        "            bottleneck_output = self.bn_function(prev_features)\n",
        "\n",
        "        new_features = self.conv2(bottleneck_output)\n",
        "\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "\n",
        "        return new_features\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int,\n",
        "        num_input_features: int,\n",
        "        bn_size: int,\n",
        "        growth_rate: int,\n",
        "        drop_rate: float,\n",
        "        memory_efficient: bool = False,\n",
        "        b_exp = 2,\n",
        "        max_out = 2\n",
        "    ) -> None:\n",
        "        super(_DenseBlock, self).__init__()\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                                num_input_features + i * growth_rate,\n",
        "                                growth_rate=growth_rate,\n",
        "                                bn_size=bn_size,\n",
        "                                drop_rate=drop_rate,\n",
        "                                memory_efficient=memory_efficient,\n",
        "                                b_exp=b_exp,\n",
        "                                max_out=max_out\n",
        "                                )\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features: Tensor) -> Tensor:\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features: int, num_output_features: int, b_exp=2, max_out=2) -> None:\n",
        "        super(_Transition, self).__init__()\n",
        "        self.conv = BcosConv2d(num_input_features, num_output_features, kernel_size=1, stride=1, padding=0, b=b_exp, max_out=max_out)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    r\"\"\"Densenet-BC model class, based on\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\n",
        "\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        growth_rate: int = 32,\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n",
        "        num_init_features: int = 64,\n",
        "        bn_size: int = 4,\n",
        "        drop_rate: float = 0,\n",
        "        num_classes: int = 10,\n",
        "        memory_efficient: bool = False,\n",
        "        b_exp = 2,\n",
        "        max_out = 2\n",
        "    ) -> None:\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "                                                  ('conv0', BcosConv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, b=b_exp, max_out=max_out)),\n",
        "                                                  ('pool0', nn.AvgPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "                                                  ]))\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "\n",
        "            block = _DenseBlock(\n",
        "                                  num_layers=num_layers,\n",
        "                                  num_input_features=num_features,\n",
        "                                  bn_size=bn_size,\n",
        "                                  growth_rate=growth_rate,\n",
        "                                  drop_rate=drop_rate,\n",
        "                                  memory_efficient=memory_efficient,\n",
        "                                  b_exp=b_exp,\n",
        "                                  max_out=max_out\n",
        "                                )\n",
        "\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2, b_exp=b_exp, max_out=max_out)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        self.classifier = BcosConv2d(num_features, num_classes, kernel_size=1, stride=1, padding=0, b=b_exp, max_out=max_out)\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def get_features(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "    def get_sequential_model(self):\n",
        "        model = nn.Sequential(*[m for m in self.features], self.classifier)\n",
        "        return model\n",
        "\n",
        "    def get_layer_idx(self, idx):\n",
        "        return idx\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        features = self.features(x)\n",
        "        out = self.classifier(features)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        return out\n",
        "\n",
        "\n",
        "def _load_state_dict(model: nn.Module, model_url: str, progress: bool) -> None:\n",
        "    # '.'s are no longer allowed in module names, but previous _DenseLayer\n",
        "    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "    # They are also in the checkpoints in model_urls. This pattern is used\n",
        "    # to find such keys.\n",
        "\n",
        "    pattern = re.compile(r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "\n",
        "    state_dict = load_state_dict_from_url(model_url, progress=progress)\n",
        "    for key in list(state_dict.keys()):\n",
        "        res = pattern.match(key)\n",
        "        if res:\n",
        "            new_key = res.group(1) + res.group(2)\n",
        "            state_dict[new_key] = state_dict[key]\n",
        "            del state_dict[key]\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def _densenet(\n",
        "    arch: str,\n",
        "    growth_rate: int,\n",
        "    block_config: Tuple[int, int, int, int],\n",
        "    num_init_features: int,\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    **kwargs: Any\n",
        ") -> DenseNet:\n",
        "\n",
        "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        _load_state_dict(model, model_url[arch], progress)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet121(pretrained: bool = False, progress: bool = True, num_init_features=64, growth_rate=32, **kwargs: Any) -> DenseNet:\n",
        "\n",
        "    r\"\"\"Densenet-121 model from \"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>_.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet.\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr.\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient, but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\n",
        "    \"\"\"\n",
        "\n",
        "    return _densenet('densenet121', growth_rate, (6, 12, 24, 16), num_init_features, pretrained, progress, **kwargs)"
      ],
      "metadata": {
        "id": "p5VV-BxWHDzf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### InceptionNet (v3)"
      ],
      "metadata": {
        "id": "vNJgU7lcKco_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['Inception3','InceptionOutputs', '_InceptionOutputs', 'inception_v3']\n",
        "\n",
        "model_url = {'inception_v3_google': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'}\n",
        "\n",
        "InceptionOutputs = namedtuple('InceptionOutputs', ['logits', 'aux_logits'])\n",
        "InceptionOutputs.__annotations__ = {'logits': Tensor, 'aux_logits': Optional[Tensor]}\n",
        "\n",
        "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
        "# _InceptionOutputs set here for backwards compat\n",
        "_InceptionOutputs = InceptionOutputs\n",
        "\n",
        "\n",
        "class Inception3(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 10,\n",
        "        aux_logits: bool = False,\n",
        "        transform_input: bool = False,\n",
        "        inception_blocks: Optional[List[Callable[..., nn.Module]]] = None,\n",
        "        init_weights: Optional[bool] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(Inception3, self).__init__()\n",
        "\n",
        "        if inception_blocks is None:\n",
        "            inception_blocks = [BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE, InceptionAux]\n",
        "\n",
        "        if init_weights is None:\n",
        "            warnings.warn('The default weight initialization of inception_v3 will be changed in future releases of '\n",
        "                          'torchvision. If you wish to keep the old behavior (which leads to long initialization times'\n",
        "                          ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
        "            init_weights = True\n",
        "\n",
        "        assert len(inception_blocks) == 7\n",
        "        conv_block = inception_blocks[0]\n",
        "        inception_a = inception_blocks[1]\n",
        "        inception_b = inception_blocks[2]\n",
        "        inception_c = inception_blocks[3]\n",
        "        inception_d = inception_blocks[4]\n",
        "        inception_e = inception_blocks[5]\n",
        "        inception_aux = inception_blocks[6]\n",
        "\n",
        "        self.aux_logits = aux_logits\n",
        "        self.transform_input = transform_input\n",
        "\n",
        "        self.Conv2d_1a_3x3 = conv_block(3, 32, kernel_size=3, stride=2)\n",
        "        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n",
        "        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n",
        "        self.avgpool1 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n",
        "        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n",
        "        self.avgpool2 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.Mixed_5b = inception_a(192, pool_features=32)\n",
        "        self.Mixed_5c = inception_a(256, pool_features=64)\n",
        "        self.Mixed_5d = inception_a(288, pool_features=64)\n",
        "\n",
        "        self.Mixed_6a = inception_b(288)\n",
        "        self.Mixed_6b = inception_c(768, channels_7x7=128)\n",
        "        self.Mixed_6c = inception_c(768, channels_7x7=160)\n",
        "        self.Mixed_6d = inception_c(768, channels_7x7=160)\n",
        "        self.Mixed_6e = inception_c(768, channels_7x7=192)\n",
        "\n",
        "        self.AuxLogits: Optional[nn.Module] = None\n",
        "        if aux_logits:\n",
        "            self.AuxLogits = inception_aux(768, num_classes)\n",
        "\n",
        "        self.Mixed_7a = inception_d(768)\n",
        "        self.Mixed_7b = inception_e(1280)\n",
        "        self.Mixed_7c = inception_e(2048)\n",
        "\n",
        "        self.fc = BcosConv2d(2048, num_classes, kernel_size=1, stride=1, padding=0, scale_fact=200)\n",
        "\n",
        "        self.debug = False\n",
        "\n",
        "        if init_weights:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                    import scipy.stats as stats\n",
        "                    stddev = m.stddev if hasattr(m, 'stddev') else 0.1\n",
        "                    X = stats.truncnorm(-2, 2, scale=stddev)\n",
        "                    values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n",
        "                    values = values.view(m.weight.size())\n",
        "                    with torch.no_grad():\n",
        "                        m.weight.copy_(values)\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def get_features(self, x):\n",
        "        return self.get_sequential_model()[:-1](x)\n",
        "\n",
        "    def _transform_input(self, x: Tensor) -> Tensor:\n",
        "        return x\n",
        "\n",
        "    def get_sequential_model(self):\n",
        "        \"\"\" For evaluation purposes only, to extract layers at roughly the same relative network depth between different models. \"\"\"\n",
        "        model = nn.Sequential(\n",
        "                                self.Conv2d_1a_3x3,\n",
        "                                self.Conv2d_2a_3x3,\n",
        "                                self.Conv2d_2b_3x3,\n",
        "                                self.avgpool1,\n",
        "                                self.Conv2d_3b_1x1,\n",
        "                                self.Conv2d_4a_3x3,\n",
        "                                self.avgpool2,\n",
        "                                self.Mixed_5b,\n",
        "                                self.Mixed_5c,\n",
        "                                self.Mixed_5d,\n",
        "                                self.Mixed_6a,\n",
        "                                self.Mixed_6b,\n",
        "                                self.Mixed_6c,\n",
        "                                self.Mixed_6d,\n",
        "                                self.Mixed_6e,\n",
        "                                self.Mixed_7a,\n",
        "                                self.Mixed_7b,\n",
        "                                self.Mixed_7c,\n",
        "                                self.fc\n",
        "                              )\n",
        "        return model\n",
        "\n",
        "    def get_layer_idx(self, idx):\n",
        "        \"\"\" For evaluation purposes only, to extract layers at roughly the same relative network depth between different models. \"\"\"\n",
        "        return int(np.ceil(len(self.get_sequential_model())*idx/10))\n",
        "\n",
        "    def print(self, layer_name, x):\n",
        "        if self.debug:\n",
        "            print(layer_name, x.shape)\n",
        "\n",
        "    def _forward(self, x: Tensor):\n",
        "        # N x 3 x 299 x 299\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # N x 32 x 149 x 149\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # N x 32 x 147 x 147\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # N x 64 x 147 x 147\n",
        "        x = self.avgpool1(x)\n",
        "        # N x 64 x 73 x 73\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # N x 80 x 73 x 73\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # N x 192 x 71 x 71\n",
        "        x = self.avgpool2(x)\n",
        "        # N x 192 x 35 x 35\n",
        "        x = self.Mixed_5b(x)\n",
        "        # N x 256 x 35 x 35\n",
        "        x = self.Mixed_5c(x)\n",
        "        # N x 288 x 35 x 35\n",
        "        x = self.Mixed_5d(x)\n",
        "        # N x 288 x 35 x 35\n",
        "        x = self.Mixed_6a(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6b(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6c(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6d(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6e(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        aux: Optional[Tensor] = None\n",
        "        if self.AuxLogits is not None:\n",
        "            if self.training:\n",
        "                self.aux_out = self.AuxLogits(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_7a(x)\n",
        "        # N x 1280 x 8 x 8\n",
        "        x = self.Mixed_7b(x)\n",
        "        # N x 2048 x 8 x 8\n",
        "        x = self.Mixed_7c(x)\n",
        "        # N x 2048 x 1 x 1\n",
        "        x = self.fc(x)\n",
        "        # N x 1000 (num_classes)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        return x\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def eager_outputs(self, x: Tensor, aux: Optional[Tensor]):\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> InceptionOutputs:\n",
        "        x = self._transform_input(x)\n",
        "        x = self._forward(x)\n",
        "        aux_defined = self.training and self.aux_logits\n",
        "        if torch.jit.is_scripting():\n",
        "            if not aux_defined:\n",
        "                warnings.warn(\"Scripted Inception3 always returns Inception3 Tuple\")\n",
        "            return InceptionOutputs(x, None)\n",
        "        else:\n",
        "            return self.eager_outputs(x, None)\n",
        "\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        pool_features: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionA, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "\n",
        "        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "\n",
        "        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n",
        "        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = self.pool(x)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionB(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionB, self).__init__()\n",
        "\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "\n",
        "        self.branch3x3 = conv_block(in_channels, 384, kernel_size=1, stride=2)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=1, stride=2)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = self.pool(x)\n",
        "\n",
        "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionC(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        channels_7x7: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionC, self).__init__()\n",
        "\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "        c7 = channels_7x7\n",
        "        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n",
        "        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n",
        "        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        branch_pool = self.pool(x)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionD(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionD, self).__init__()\n",
        "\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "\n",
        "        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "        self.branch3x3_2 = conv_block(192, 320, kernel_size=1, stride=2)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=1, stride=2)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = self.branch3x3_2(branch3x3)\n",
        "\n",
        "        branch7x7x3 = self.branch7x7x3_1(x)\n",
        "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
        "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
        "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
        "\n",
        "        branch_pool = self.pool(x)\n",
        "\n",
        "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionE(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionE, self).__init__()\n",
        "\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n",
        "\n",
        "        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n",
        "        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "                      self.branch3x3_2a(branch3x3),\n",
        "                      self.branch3x3_2b(branch3x3),\n",
        "                    ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "                          self.branch3x3dbl_3a(branch3x3dbl),\n",
        "                          self.branch3x3dbl_3b(branch3x3dbl),\n",
        "                       ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        branch_pool = self.pool(x)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionAux(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_classes: int,\n",
        "        conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "\n",
        "        super(InceptionAux, self).__init__()\n",
        "\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "\n",
        "        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
        "\n",
        "        self.conv1 = conv_block(128, 768, kernel_size=5)\n",
        "        self.conv1.stddev = 0.01\n",
        "\n",
        "        self.fc = BcosConv2d(768, num_classes, kernel_size=1, stride=1, padding=0, scale_fact=200)\n",
        "        self.fc.stddev = 0.001\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.pool(x)\n",
        "        # N x 768 x 5 x 5\n",
        "        x = self.conv0(x)\n",
        "        # N x 128 x 5 x 5\n",
        "        x = self.conv1(x)\n",
        "        x = self.fc(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))[..., 0, 0]\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "\n",
        "        super(BasicConv2d, self).__init__()\n",
        "\n",
        "        self.conv = BcosConv2d(in_channels, out_channels, scale_fact=200, **kwargs)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "def inception_v3(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> \"Inception3\":\n",
        "\n",
        "    r\"\"\"Inception v3 model architecture from \"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>_.\n",
        "\n",
        "    .. note:: **Important**: In contrast to the other models the inception_v3 expects tensors with a size of N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet.\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr.\n",
        "        aux_logits (bool): If True, add an auxiliary branch that can improve training. Default: *True*.\n",
        "        transform_input (bool): If True, preprocesses the input according to the method with which it was trained on ImageNet. Default: *False*.\n",
        "    \"\"\"\n",
        "\n",
        "    if pretrained:\n",
        "\n",
        "        if 'transform_input' not in kwargs:\n",
        "            kwargs['transform_input'] = True\n",
        "\n",
        "        if 'aux_logits' in kwargs:\n",
        "            original_aux_logits = kwargs['aux_logits']\n",
        "            kwargs['aux_logits'] = True\n",
        "        else:\n",
        "            original_aux_logits = True\n",
        "\n",
        "        kwargs['init_weights'] = False  # We are loading weights from a pretrained model.\n",
        "\n",
        "        model = Inception3(**kwargs)\n",
        "\n",
        "        state_dict = load_state_dict_from_url(model_url['inception_v3_google'], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "        if not original_aux_logits:\n",
        "            model.aux_logits = False\n",
        "            model.AuxLogits = None\n",
        "        return model\n",
        "\n",
        "    return Inception3(**kwargs)"
      ],
      "metadata": {
        "id": "qbXLkVsxGzQB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "3_CTvJcVWNO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Training**\n",
        "\n",
        "We trained our models for 5 epochs with Adam, an initial learning rate of 0.001 and a batch size of 64.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IaADtvEoWX9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getArgs():\n",
        "    parser = argparse.ArgumentParser(description=\"BCOS_TRAINING\")\n",
        "    parser.add_argument('-f')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    args.batch_size = 64\n",
        "    args.dataset = 'CIFAR10'\n",
        "    args.epochs = 15\n",
        "    args.learning_rate = 0.001\n",
        "    args.model_name = 'ResNet34'\n",
        "    args.model = ResNet34()\n",
        "\n",
        "    args.cifar10Path = '/var/datasets/CIFAR10'\n",
        "\n",
        "    args.save_losses = '/content/'\n",
        "    args.save_ckpt = '/content/ckpt/'\n",
        "\n",
        "    args.load_ckpt = f'/content/ckpt/Epoch_{args.epochs}.pt'\n",
        "    args.save_results_path = '/content/'\n",
        "\n",
        "    return args"
      ],
      "metadata": {
        "id": "SbkVxDB1grJk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Optimising B-cos networks for classification**\n",
        "\n",
        "*   First, note that the output of each neuron is bounded.\n",
        "Since the output of a B-cos network is computed as a sequence of such bounded transforms, the output of the network as a whole is also bounded.\n",
        "\n",
        "*   Secondly, note that a B-cos network as a whole can only achieve its upper bound for a given input if the units in each layer achieve their upper bound.\n",
        "The individual units, in turn, can only achieve their maxima by aligning with their inputs.\n",
        "Hence, optimising a B-cos network to maximise its output over a set of inputs will optimise the model weights to align with those inputs.\n",
        "\n",
        "In order to take advantage of this when optimising for classification, we train the B-cos networks with the **Binary Cross Entropy** (**BCE**) loss:\n",
        "\n",
        "      L(xᵢ, yᵢ) = BCE(σ(f(xᵢ; θ) + b), yᵢ)`,\n",
        "for input xᵢ and its corresponding one-hot encoded class label yᵢ.\n",
        "Here, σ denotes the sigmoid function, b a bias, and θ the model parameters.\n",
        "\n",
        "We choose the BCE loss because it directly entails output maximisation. Specifically, in order to reduce the BCE loss, the network is optimised to maximise the (negative) class logit for the correct (incorrect) classes.\n",
        "\n",
        "Finally, note that increasing B allows to specifically reduce the output of badly aligned weights in each layer.\n",
        "This will decrease the layer’s output strength and thus the output of the network as a whole for badly aligned weights, which increases the alignment pressure during optimisation (thus, **higher B** → **higher alignment**).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fYYuX1KHWTw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        self.loader = loadData(args)\n",
        "        self.model = args.model\n",
        "        self.create_paths(args.save_ckpt, args.save_losses)\n",
        "\n",
        "    @staticmethod\n",
        "    def create_paths(ckpt_path, losses_path):\n",
        "\n",
        "        if not os.path.exists(ckpt_path):\n",
        "            os.makedirs(ckpt_path)\n",
        "\n",
        "        if not os.path.exists(losses_path):\n",
        "            os.makedirs(losses_path)\n",
        "\n",
        "    def training(self, args):\n",
        "\n",
        "        # LOADING DATA #\n",
        "        train_dataloader, val_dataloader = self.loader.getDataLoader()\n",
        "\n",
        "        # TRAINING PARAMETERS #\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "        all_train_loss = []\n",
        "        all_val_loss = []\n",
        "\n",
        "        # print(self.model)\n",
        "\n",
        "        #############################\n",
        "        #       TRAINING LOOP       #\n",
        "        #############################\n",
        "\n",
        "        print(f'\\nSTARTING TRAINING WITH {args.model_name} FOR {args.dataset}')\n",
        "\n",
        "        for epoch in range(args.epochs):\n",
        "            epoch_train_loss = []\n",
        "            epoch_val_loss = []\n",
        "\n",
        "            self.model.train()\n",
        "            for imgs, labels in tqdm(train_dataloader):\n",
        "                labels = F.one_hot(labels, num_classes=10)\n",
        "\n",
        "                logits = self.model(imgs)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                loss = criterion(logits, labels.float())\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_train_loss.append(loss.detach().numpy())\n",
        "\n",
        "            self.model.eval()\n",
        "            for imgs, labels in tqdm(val_dataloader):\n",
        "                labels = F.one_hot(labels, num_classes=10)\n",
        "\n",
        "                logits = self.model(imgs)\n",
        "\n",
        "                loss = criterion(logits, labels.float())\n",
        "\n",
        "                epoch_val_loss.append(loss.detach().numpy())\n",
        "\n",
        "            train_loss, val_loss = np.mean(epoch_train_loss), np.mean(epoch_val_loss)\n",
        "\n",
        "            all_train_loss.append(train_loss)\n",
        "            all_val_loss.append(val_loss)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                np.save(os.path.join(args.save_losses, 'train_loss.npy'), all_train_loss)\n",
        "                np.save(os.path.join(args.save_losses, 'val_loss.npy'), all_val_loss)\n",
        "                torch.save(self.model.state_dict(), os.path.join(args.save_ckpt, f'Epoch_{args.epochs}.pt'))\n",
        "\n",
        "        print(f'\\n--- FINISHED {args.model_name} TRAINING ---')\n",
        "\n",
        "\n",
        "args = getArgs()\n",
        "\n",
        "# TRAINING\n",
        "Trainer(args).training(args)"
      ],
      "metadata": {
        "id": "Xej-ZNpO3I1U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "535d14c5-228d-46aa-c493-e5aa33cc7384"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "STARTING TRAINING WITH ResNet34 FOR CIFAR10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 79/782 [00:54<08:07,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-548f47ccc44a>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-548f47ccc44a>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f9001dc1639a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f9001dc1639a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c6b708e1a507>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout_normed_conv2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormedConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_normed_conv2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c6b708e1a507>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mw_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mw_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_hat\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mw_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1515\u001b[0m             \u001b[0m_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1517\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1518\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "ILJymRTnYXKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Evaluating explanations**\n",
        "\n",
        "We evaluate the B-cos networks across all models to investigate which one provides the best explanation.\n",
        "\n",
        "This makes it possible to compare explanations between different models and to evaluate the explainability gain achieved by converting conventional models to B-cos networks.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0HnBFe91qNdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getResults(y_true, y_pred):\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "    dict_metrics = {'accuracy' : [accuracy],\n",
        "                    'f1-score' : [f1],\n",
        "                    'precision' : [precision],\n",
        "                    'recall' : [recall]}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dict_metrics)\n",
        "\n",
        "    print('\\n\\n-- CONFUSION MATRIX --\\n')\n",
        "    print(conf_matrix)\n",
        "\n",
        "    print('\\n-- CLASSIFICATION METRICS --\\n')\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    return dict_metrics\n",
        "\n",
        "\n",
        "# From the original implementation\n",
        "def getExplanationImage(img, grads, smooth=15, alpha_percent=99.5):\n",
        "\n",
        "    contribution = (img*grads).sum(0, keepdim=True)\n",
        "    # print('contribution', contribution.shape)\n",
        "\n",
        "    rgb_grad = (grads / (grads.abs().max(0, keepdim=True)[0] + 1e-12))\n",
        "    rgb_grad = rgb_grad.clamp(0).numpy()\n",
        "    # print('rgb_grad', rgb_grad.shape)\n",
        "\n",
        "    alpha = (grads.norm(p=2, dim=0, keepdim=True))\n",
        "    # print('alpha', alpha.shape)\n",
        "\n",
        "    # Only show positive contributions\n",
        "    alpha = torch.where(contribution < 0, torch.zeros_like(alpha) + 1e-12, alpha)\n",
        "\n",
        "    if smooth:\n",
        "        alpha = F.avg_pool2d(alpha, smooth, stride=1, padding=(smooth-1)//2)\n",
        "\n",
        "    alpha = alpha.numpy()\n",
        "    alpha = (alpha / np.percentile(alpha, alpha_percent)).clip(0, 1)\n",
        "    # print('alpha', alpha.shape)\n",
        "\n",
        "    rgb_grad = np.concatenate([rgb_grad, alpha], axis=0)[1:]\n",
        "    # print('rgb_grad', rgb_grad.shape)\n",
        "\n",
        "    # Reshaping to [H, W, C]\n",
        "    grad_image = rgb_grad.transpose((1, 2, 0))\n",
        "    grad_image = torch.tensor(grad_image)\n",
        "    grad_image = grad_image.permute(2, 0, 1)\n",
        "    # print('grad_image', grad_image.shape)\n",
        "\n",
        "    return grad_image\n",
        "\n",
        "\n",
        "class Inference:\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        self.loader = loadData(args)\n",
        "        self.model = self.getModels(args)\n",
        "\n",
        "    def getModels(self, args):\n",
        "\n",
        "        # LOADING MODEL\n",
        "        model = args.model\n",
        "\n",
        "        if args.load_ckpt is not None:\n",
        "            path = args.load_ckpt.split('ckpt/')[-1]\n",
        "            print(f'LOADING MODEL: {path}\\n')\n",
        "            model.load_state_dict(torch.load(args.load_ckpt), strict=False)\n",
        "        else:\n",
        "            print('LOADING MODEL: no checkpoint -> initializing randomly\\n')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def evaluateMetrics(self, args):\n",
        "\n",
        "        # LOADING DATA #\n",
        "        train_dataloader, val_dataloader = self.loader.getDataLoader()\n",
        "\n",
        "        # TRAINING PARAMETERS #\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        #############################\n",
        "        #       INFERENCE LOOP      #\n",
        "        #############################\n",
        "\n",
        "        print(f'\\nSTARTING CLASSIFICATION WITH {args.model_name} FOR {args.dataset}')\n",
        "\n",
        "        all_pred = []\n",
        "        all_labels = []\n",
        "\n",
        "        self.model.eval()\n",
        "        for imgs, labels in tqdm(val_dataloader):\n",
        "\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            logits = self.model(imgs)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            max_values, pred = torch.max(probs, dim=-1)\n",
        "\n",
        "            pred, labels = list(pred.detach().numpy()), list(labels.detach().numpy())\n",
        "            all_pred = np.concatenate((all_pred, pred))\n",
        "            all_labels = np.concatenate((all_labels, labels))\n",
        "\n",
        "        all_pred = torch.tensor(all_pred)\n",
        "        all_labels = torch.tensor(all_labels)\n",
        "\n",
        "        getResults(all_labels, all_pred)\n",
        "\n",
        "    def getExplanations(self, args):\n",
        "\n",
        "        # LOADING DATA #\n",
        "        train_dataloader, val_dataloader = self.loader.getDataLoader()\n",
        "\n",
        "        # TRAINING PARAMETERS #\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        #############################\n",
        "        #       INFERENCE LOOP      #\n",
        "        #############################\n",
        "\n",
        "        print(f'\\nSTARTING EXPLANATIONS GENERATION WITH {args.model_name} FOR {args.dataset}')\n",
        "\n",
        "        ori_images = []\n",
        "        exp_images = []\n",
        "\n",
        "        self.model.eval()\n",
        "        for imgs, labels in tqdm(val_dataloader):\n",
        "\n",
        "            # from the original implementation\n",
        "            imgs = imgs.requires_grad_(True)\n",
        "\n",
        "            logits = self.model(imgs)\n",
        "            max_logit = self.model(imgs).max()\n",
        "\n",
        "            imgs.grad = None\n",
        "            self.model.zero_grad()\n",
        "            max_logit.backward()\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            max_values, max_indices = torch.max(logits, dim=-1)\n",
        "            max_probs, max_img_in_batch = torch.max(max_values, dim=0)\n",
        "\n",
        "            explanation = getExplanationImage(imgs[max_img_in_batch], imgs.grad[max_img_in_batch])\n",
        "\n",
        "            ori_images.append(imgs[max_img_in_batch])\n",
        "            exp_images.append(explanation)\n",
        "\n",
        "        ori_images = torch.stack(ori_images)\n",
        "        # print(ori_images.shape)\n",
        "\n",
        "        exp_images = torch.stack(exp_images)\n",
        "        # print(exp_images.shape)\n",
        "\n",
        "        real_fake_images = torch.cat((ori_images[:5], exp_images.add(1).mul(0.5)[:5]))\n",
        "        utils.save_image(real_fake_images, os.path.join(args.save_results_path, 'explanation_results.jpg'), nrow=5)\n",
        "\n",
        "        print(f'\\n\\n--- FINISHED {args.model_name} EXPLANATIONS GENERATION ---')\n",
        "\n",
        "\n",
        "args = getArgs()\n",
        "\n",
        "# Inference(args).evaluateMetrics(args)\n",
        "Inference(args).getExplanations(args)"
      ],
      "metadata": {
        "id": "wC_XkV8mQEuR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}